{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeezgqhJoTSA"
      },
      "source": [
        "# VGG for Dog Emotion Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9KrTzc4oYzt"
      },
      "source": [
        "## Load Library and Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RHR7YQloPOt",
        "outputId": "88e1083c-e415-4077-89b0-d5d3d8cdab19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x1ff88023d50>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "# from google.colab import drive \n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "# import os\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "# Load Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "# os.chdir('/content/drive/MyDrive/Final_Project')\n",
        "\n",
        "# Setting Seed\n",
        "torch.manual_seed(1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr9TPIRFoqw-"
      },
      "source": [
        "## Loading Data with Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s6I0egNRotdW"
      },
      "outputs": [],
      "source": [
        "def data_loader(data_dir, batch_size, shuffle = True): \n",
        "  # data augumentation\n",
        "  transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "      mean=[0.485, 0.456, 0.406],\n",
        "      std=[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(degrees = (-90, 90)),\n",
        "    # transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 1))\n",
        "  ])\n",
        "\n",
        "  \n",
        "  dataset = datasets.ImageFolder(data_dir, transform = transform)\n",
        "  # classes = dataset.classes\n",
        "\n",
        "  # Split data into Training and Testing Set\n",
        "  data_split = torch.utils.data.random_split(dataset, lengths = [0.95, 0.05])\n",
        "  dataset_train = data_split[0]\n",
        "  dataset_test = data_split[1]\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle = shuffle)\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle = shuffle)\n",
        "\n",
        "  return (train_loader, test_loader)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_loader, test_loader = data_loader('./Data', batch_size = 13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPRBMf0sYTKp"
      },
      "source": [
        "## Defining Pretrain Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvHiTq-1YX6X"
      },
      "outputs": [],
      "source": [
        "vgg16 = models.vgg16_bn(weights=models.VGG16_BN_Weights.DEFAULT)\n",
        "\n",
        "# Original pretrained VGG model\n",
        "vgg16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qht1VYoyZ-6t",
        "outputId": "b286b44b-439c-4671-c69d-da231ec48963"
      },
      "outputs": [],
      "source": [
        "# Modified Pretrained Model\n",
        "in_features = vgg16.classifier[6].in_features\n",
        "layers = list(vgg16.classifier.children())[:-1] # Remove last layer\n",
        "layers.extend([nn.Linear(in_features, 4)]) # Add our layer with 4 outputs\n",
        "# Randomly initialize weights and biases\n",
        "bound = 1/math.sqrt(layers[0].in_features)\n",
        "nn.init.kaiming_uniform_(layers[0].weight)\n",
        "nn.init.uniform_(layers[0].bias, -bound, bound)\n",
        "\n",
        "bound = 1/math.sqrt(layers[3].in_features)\n",
        "nn.init.kaiming_uniform_(layers[3].weight)\n",
        "nn.init.uniform_(layers[3].bias, -bound, bound)\n",
        "\n",
        "bound = 1/math.sqrt(layers[6].in_features)\n",
        "nn.init.kaiming_uniform_(layers[6].weight)\n",
        "nn.init.uniform_(layers[6].bias, -bound, bound)\n",
        "\n",
        "vgg16.classifier = nn.Sequential(*layers) # Replace the model classifier\n",
        "print(vgg16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (12): ReLU(inplace=True)\n",
              "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (16): ReLU(inplace=True)\n",
              "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (19): ReLU(inplace=True)\n",
              "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (26): ReLU(inplace=True)\n",
              "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (32): ReLU(inplace=True)\n",
              "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (36): ReLU(inplace=True)\n",
              "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (39): ReLU(inplace=True)\n",
              "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (42): ReLU(inplace=True)\n",
              "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=4, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Insert code to load previously trained models on the same dataset\n",
        "# Needed due to limited computation power\n",
        "vgg16 = torch.load('./trained_models/vgg16_dog_emotion_best.pt')\n",
        "vgg16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKYV7vd9Z_hC",
        "outputId": "3129529f-51c5-456e-c7e7-e1248b8d3074"
      },
      "outputs": [],
      "source": [
        "# Freeze all feature layers\n",
        "for param in vgg16.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# test code for viewing model parameters\n",
        "# for name, param in vgg16.named_parameters():\n",
        "#     if param.requires_grad:\n",
        "#         print(name, param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Move Model to GPU\n",
        "vgg16.train()\n",
        "vgg16 = vgg16.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPfcvjSKdOEd"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TFWuMD4odSq-"
      },
      "outputs": [],
      "source": [
        "num_epochs = 50\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(vgg16.classifier.parameters(), lr=learning_rate, weight_decay = 0, momentum = 0.9)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgxlULpHfbep"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zXSlA8SVfdcN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Step [0/1456], Loss: 0.7783\n",
            "Epoch [1/50], Step [100/1456], Loss: 1.0850\n",
            "Epoch [1/50], Step [200/1456], Loss: 1.3454\n",
            "Epoch [1/50], Step [300/1456], Loss: 1.2907\n",
            "Epoch [1/50], Step [400/1456], Loss: 0.6299\n",
            "Epoch [1/50], Step [500/1456], Loss: 0.9643\n",
            "Epoch [1/50], Step [600/1456], Loss: 0.9154\n",
            "Epoch [1/50], Step [700/1456], Loss: 1.0268\n",
            "Epoch [1/50], Step [800/1456], Loss: 0.8327\n",
            "Epoch [1/50], Step [900/1456], Loss: 0.9072\n",
            "Epoch [1/50], Step [1000/1456], Loss: 0.8800\n",
            "Epoch [1/50], Step [1100/1456], Loss: 0.7284\n",
            "Epoch [1/50], Step [1200/1456], Loss: 0.6914\n",
            "Epoch [1/50], Step [1300/1456], Loss: 0.5950\n",
            "Epoch [1/50], Step [1400/1456], Loss: 1.2101\n",
            "Accuracy of the network on training images: 61.55878467635403 %\n",
            "Epoch [2/50], Step [0/1456], Loss: 1.2259\n",
            "Epoch [2/50], Step [100/1456], Loss: 1.0910\n",
            "Epoch [2/50], Step [200/1456], Loss: 1.0039\n",
            "Epoch [2/50], Step [300/1456], Loss: 0.8493\n",
            "Epoch [2/50], Step [400/1456], Loss: 1.3828\n",
            "Epoch [2/50], Step [500/1456], Loss: 1.0775\n",
            "Epoch [2/50], Step [600/1456], Loss: 0.9709\n",
            "Epoch [2/50], Step [700/1456], Loss: 1.0452\n",
            "Epoch [2/50], Step [800/1456], Loss: 1.2198\n",
            "Epoch [2/50], Step [900/1456], Loss: 1.0207\n",
            "Epoch [2/50], Step [1000/1456], Loss: 1.0496\n",
            "Epoch [2/50], Step [1100/1456], Loss: 0.8856\n",
            "Epoch [2/50], Step [1200/1456], Loss: 0.8165\n",
            "Epoch [2/50], Step [1300/1456], Loss: 0.7395\n",
            "Epoch [2/50], Step [1400/1456], Loss: 1.1452\n",
            "Accuracy of the network on training images: 61.27873183619551 %\n",
            "Epoch [3/50], Step [0/1456], Loss: 0.9162\n",
            "Epoch [3/50], Step [100/1456], Loss: 0.6123\n",
            "Epoch [3/50], Step [200/1456], Loss: 0.8352\n",
            "Epoch [3/50], Step [300/1456], Loss: 1.0249\n",
            "Epoch [3/50], Step [400/1456], Loss: 0.9761\n",
            "Epoch [3/50], Step [500/1456], Loss: 0.9765\n",
            "Epoch [3/50], Step [600/1456], Loss: 1.3650\n",
            "Epoch [3/50], Step [700/1456], Loss: 0.6674\n",
            "Epoch [3/50], Step [800/1456], Loss: 0.7161\n",
            "Epoch [3/50], Step [900/1456], Loss: 0.8008\n",
            "Epoch [3/50], Step [1000/1456], Loss: 0.6357\n",
            "Epoch [3/50], Step [1100/1456], Loss: 1.3015\n",
            "Epoch [3/50], Step [1200/1456], Loss: 0.7981\n",
            "Epoch [3/50], Step [1300/1456], Loss: 0.9408\n",
            "Epoch [3/50], Step [1400/1456], Loss: 1.2493\n",
            "Accuracy of the network on training images: 62.081902245706736 %\n",
            "Epoch [4/50], Step [0/1456], Loss: 1.3923\n",
            "Epoch [4/50], Step [100/1456], Loss: 0.9054\n",
            "Epoch [4/50], Step [200/1456], Loss: 1.1765\n",
            "Epoch [4/50], Step [300/1456], Loss: 1.1411\n",
            "Epoch [4/50], Step [400/1456], Loss: 1.2243\n",
            "Epoch [4/50], Step [500/1456], Loss: 1.0262\n",
            "Epoch [4/50], Step [600/1456], Loss: 0.9272\n",
            "Epoch [4/50], Step [700/1456], Loss: 0.6680\n",
            "Epoch [4/50], Step [800/1456], Loss: 0.7670\n",
            "Epoch [4/50], Step [900/1456], Loss: 1.5129\n",
            "Epoch [4/50], Step [1000/1456], Loss: 0.9840\n",
            "Epoch [4/50], Step [1100/1456], Loss: 1.2235\n",
            "Epoch [4/50], Step [1200/1456], Loss: 0.6174\n",
            "Epoch [4/50], Step [1300/1456], Loss: 0.9457\n",
            "Epoch [4/50], Step [1400/1456], Loss: 1.2118\n",
            "Accuracy of the network on training images: 61.73844121532365 %\n",
            "Epoch [5/50], Step [0/1456], Loss: 1.0226\n",
            "Epoch [5/50], Step [100/1456], Loss: 0.6733\n",
            "Epoch [5/50], Step [200/1456], Loss: 0.7605\n",
            "Epoch [5/50], Step [300/1456], Loss: 0.9626\n",
            "Epoch [5/50], Step [400/1456], Loss: 1.0659\n",
            "Epoch [5/50], Step [500/1456], Loss: 1.1689\n",
            "Epoch [5/50], Step [600/1456], Loss: 0.9355\n",
            "Epoch [5/50], Step [700/1456], Loss: 0.8478\n",
            "Epoch [5/50], Step [800/1456], Loss: 0.7504\n",
            "Epoch [5/50], Step [900/1456], Loss: 0.9776\n",
            "Epoch [5/50], Step [1000/1456], Loss: 0.9864\n",
            "Epoch [5/50], Step [1100/1456], Loss: 1.0722\n",
            "Epoch [5/50], Step [1200/1456], Loss: 0.5183\n",
            "Epoch [5/50], Step [1300/1456], Loss: 0.8517\n",
            "Epoch [5/50], Step [1400/1456], Loss: 1.1270\n"
          ]
        }
      ],
      "source": [
        "total_step = len(train_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # Move tensors to the configured device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # Forward pass\n",
        "        outputs = vgg16(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i % 100 == 0):\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i, total_step, loss.item()))\n",
        "            \n",
        "    # Validation\n",
        "    vgg16.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in train_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = vgg16(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            del images, labels, outputs\n",
        "    \n",
        "        print('Accuracy of the network on training images: {} %'.format(100 * correct / total)) \n",
        "    vgg16.train()\n",
        "    # save\n",
        "    torch.save(vgg16, f\"./trained_models/vgg16_dog_emotion_{epoch+1}.pt\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
